import os
import sys
import subprocess
import shutil
import random
from pathlib import Path
import librosa
import soundfile as sf
import numpy as np
from tqdm import tqdm
from faster_whisper import WhisperModel
import torch
import re

# Import from 'Slicer2.py'
class Slicer:
    def __init__(self,
                 sr: int,
                 threshold: float = -40.,
                 min_length: int = 5000,
                 min_interval: int = 300,
                 hop_size: int = 20,
                 max_sil_kept: int = 5000):
        if not min_length >= min_interval >= hop_size:
            raise ValueError('The following condition must be satisfied: min_length >= min_interval >= hop_size')
        if not max_sil_kept >= hop_size:
            raise ValueError('The following condition must be satisfied: max_sil_kept >= hop_size')
        min_interval = sr * min_interval / 1000
        self.threshold = 10 ** (threshold / 20.)
        self.hop_size = round(sr * hop_size / 1000)
        self.win_size = min(round(min_interval), 4 * self.hop_size)
        self.min_length = round(sr * min_length / 1000 / self.hop_size)
        self.min_interval = round(min_interval / self.hop_size)
        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)

    def _apply_slice(self, waveform, begin, end):
        if len(waveform.shape) > 1:
            return waveform[:, begin * self.hop_size: min(waveform.shape[1], end * self.hop_size)]
        else:
            return waveform[begin * self.hop_size: min(waveform.shape[0], end * self.hop_size)]

    def slice(self, waveform):
        if len(waveform.shape) > 1:
            samples = waveform.mean(axis=0)
        else:
            samples = waveform
        if (samples.shape[0] + self.hop_size - 1) // self.hop_size <= self.min_length:
            return [waveform]
        
        # Get RMS Values by Librosa
        rms_list = librosa.feature.rms(
            y=samples, 
            frame_length=self.win_size, 
            hop_length=self.hop_size
        ).squeeze(0)
        
        sil_tags = []
        silence_start = None
        clip_start = 0
        for i, rms in enumerate(rms_list):
            if rms < self.threshold:
                if silence_start is None:
                    silence_start = i
                continue
            if silence_start is None:
                continue
                
            is_leading_silence = silence_start == 0 and i > self.max_sil_kept
            need_slice_middle = i - silence_start >= self.min_interval and i - clip_start >= self.min_length
            if not is_leading_silence and not need_slice_middle:
                silence_start = None
                continue
                
            if i - silence_start <= self.max_sil_kept:
                pos = rms_list[silence_start: i + 1].argmin() + silence_start
                if silence_start == 0:
                    sil_tags.append((0, pos))
                else:
                    sil_tags.append((pos, pos))
                clip_start = pos
            elif i - silence_start <= self.max_sil_kept * 2:
                pos = rms_list[i - self.max_sil_kept: silence_start + self.max_sil_kept + 1].argmin()
                pos += i - self.max_sil_kept
                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start
                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept
                if silence_start == 0:
                    sil_tags.append((0, pos_r))
                    clip_start = pos_r
                else:
                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))
                    clip_start = max(pos_r, pos)
            else:
                pos_l = rms_list[silence_start: silence_start + self.max_sil_kept + 1].argmin() + silence_start
                pos_r = rms_list[i - self.max_sil_kept: i + 1].argmin() + i - self.max_sil_kept
                if silence_start == 0:
                    sil_tags.append((0, pos_r))
                else:
                    sil_tags.append((pos_l, pos_r))
                clip_start = pos_r
            silence_start = None
            
        total_frames = rms_list.shape[0]
        if silence_start is not None and total_frames - silence_start >= self.min_interval:
            silence_end = min(total_frames, silence_start + self.max_sil_kept)
            pos = rms_list[silence_start: silence_end + 1].argmin() + silence_start
            sil_tags.append((pos, total_frames + 1))
            
        if len(sil_tags) == 0:
            return [waveform]
        else:
            chunks = []
            if sil_tags[0][0] > 0:
                chunks.append(self._apply_slice(waveform, 0, sil_tags[0][0]))
            for i in range(len(sil_tags) - 1):
                chunks.append(self._apply_slice(waveform, sil_tags[i][1], sil_tags[i + 1][0]))
            if sil_tags[-1][1] < total_frames:
                chunks.append(self._apply_slice(waveform, sil_tags[-1][1], total_frames))
            return chunks

class AudioProcessor:
    def __init__(self, input_file, output_base="raw"):
        self.input_file = Path(input_file)
        self.base_name = self.input_file.stem
        self.output_base = Path(output_base)
        self.raw_dir = self.output_base / self.base_name
        self.wavs_dir = self.raw_dir / "wavs"
        
        # Create Dictorinary
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        self.wavs_dir.mkdir(parents=True, exist_ok=True)
        
    def run_demucs(self):
        """ä½¿ç”¨demucsåˆ†ç¦»äººå£°"""
        print("ğŸµ æ­¥éª¤1: ä½¿ç”¨Demucsåˆ†ç¦»äººå£°...")
        try:
            cmd = [
                "demucs", 
                "-n", "htdemucs", 
                "--out", "separated", 
                str(self.input_file)
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            print("âœ… Demucså¤„ç†å®Œæˆ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Demucså¤„ç†å¤±è´¥: {e}")
            raise
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°demucså‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£…demucs")
            raise
    
    def resample_audio(self):
        """ä½¿ç”¨ffmpegé‡é‡‡æ ·éŸ³é¢‘"""
        print("ğŸµ æ­¥éª¤2: é‡é‡‡æ ·éŸ³é¢‘åˆ°å•å£°é“24kHz...")
        vocals_path = Path(f"separated/htdemucs/{self.base_name}/vocals.wav")
        output_path = self.raw_dir / f"{self.base_name}.wav"
        
        if not vocals_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°åˆ†ç¦»åçš„äººå£°æ–‡ä»¶: {vocals_path}")
        
        try:
            cmd = [
                "ffmpeg", "-i", str(vocals_path),
                "-ac", "1", "-ar", "24000",
                "-y", str(output_path)
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            print("âœ… éŸ³é¢‘é‡é‡‡æ ·å®Œæˆ")
            return output_path
        except subprocess.CalledProcessError as e:
            print(f"âŒ éŸ³é¢‘é‡é‡‡æ ·å¤±è´¥: {e}")
            raise
    
    def slice_audio(self, audio_path):
        """ä½¿ç”¨librosaåˆ‡ç‰‡éŸ³é¢‘"""
        print("ğŸµ æ­¥éª¤3: åˆ‡ç‰‡éŸ³é¢‘ä¸º2-10ç§’ç‰‡æ®µ...")
        try:
            # Loading audios
            audio, sr = librosa.load(audio_path, sr=24000, mono=True)
            
            # Create Slicer
            slicer = Slicer(
                sr=sr,
                threshold=-40,
                min_length=2000,  # 2ç§’
                min_interval=300,
                hop_size=10,
                max_sil_kept=500
            )
            
            # Slice
            chunks = slicer.slice(audio)
            
            # Saving Slices
            saved_files = []
            for i, chunk in enumerate(tqdm(chunks, desc="ä¿å­˜éŸ³é¢‘åˆ‡ç‰‡")):
                if len(chunk.shape) > 1:
                    chunk = chunk.T
                
                output_file = self.wavs_dir / f"{self.base_name}_{i:03d}.wav"
                sf.write(output_file, chunk, sr)
                saved_files.append(output_file.name)
            
            print(f"âœ… éŸ³é¢‘åˆ‡ç‰‡å®Œæˆï¼Œå…±ç”Ÿæˆ {len(saved_files)} ä¸ªç‰‡æ®µ")
            return saved_files
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘åˆ‡ç‰‡å¤±è´¥: {e}")
            raise
    
    def transcribe_audio(self):
        """ä½¿ç”¨Faster-Whisperè½¬å½•éŸ³é¢‘"""
        print("ğŸµ æ­¥éª¤4: ä½¿ç”¨Whisper Large-v3è½¬å½•éŸ³é¢‘...")
        try:
            # Loading Whisper Model
            model = WhisperModel("large-v3", device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
            
            # Search for all audios
            audio_files = sorted(self.wavs_dir.glob("*.wav"))
            
            transcripts = []
            for audio_file in tqdm(audio_files, desc="è½¬å½•éŸ³é¢‘"):
                segments, info = model.transcribe(
                    str(audio_file),
                    language=None,  # Detect Language by itself
                    beam_size=5
                )
                
                text = " ".join([segment.text for segment in segments]).strip()
                # Cleaning texts
                text = re.sub(r'\s+', ' ', text)
                transcripts.append((audio_file.name, text))
            
            # Saving the transcript result
            transcript_file = self.raw_dir / "transcribed_texts.txt"
            with open(transcript_file, 'w', encoding='utf-8') as f:
                for filename, text in transcripts:
                    f.write(f"{filename}|{text}\n")
            
            print("âœ… éŸ³é¢‘è½¬å½•å®Œæˆ")
            return transcripts
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            raise
    
    def phonemize_text(self, transcripts):
        """éŸ³ç´ åŒ–æ–‡æœ¬"""
        print("ğŸµ æ­¥éª¤5: éŸ³ç´ åŒ–æ–‡æœ¬...")
        try:
            # Using espeak-ng to phonemize
            phonemized_data = []
            
            for filename, text in tqdm(transcripts, desc="éŸ³ç´ åŒ–æ–‡æœ¬"):
                if not text.strip():
                    phonemized_text = ""
                else:
                    # transform texts into phonemize
                    try:
                        cmd = ["espeak", "-q", "--phonout=-", "-v", "en", text]
                        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                        phonemized_text = result.stdout.strip()
                    except (subprocess.CalledProcessError, FileNotFoundError):
                        # å¦‚æœespeakä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„æ›¿æ¢è§„åˆ™
                        phonemized_text = self._simple_phonemize(text)
                
                phonemized_data.append((filename, phonemized_text))
            
            # Saving phonemized result into OOD_texts.txt and train_list.txt
            OOD_texts_file = self.raw_dir / "OOD_texts.txt"
            with open(OOD_texts_file, 'w', encoding='utf-8') as f:
                for filename, phonemized_text in phonemized_data:
                    f.write(f"{filename}|{phonemized_text}|0\n")


            train_list_file = self.raw_dir / "train_list.txt"
            with open(train_list_file, 'w', encoding='utf-8') as f:
                for filename, phonemized_text in phonemized_data:
                    f.write(f"{filename}|{phonemized_text}|0\n")
            
            print("âœ… æ–‡æœ¬éŸ³ç´ åŒ–å®Œæˆ")
            return phonemized_data
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬éŸ³ç´ åŒ–å¤±è´¥: {e}")
            raise
    
    def _simple_phonemize(self, text):
        """ç®€å•çš„éŸ³ç´ åŒ–æ›¿ä»£æ–¹æ¡ˆ"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„éŸ³ç´ æ˜ å°„è¡¨ï¼Œå®é™…ä½¿ç”¨æ—¶å»ºè®®ä½¿ç”¨ä¸“ä¸šçš„éŸ³ç´ åŒ–åº“
        phoneme_map = {
            'a': 'Ã¦', 'e': 'É›', 'i': 'Éª', 'o': 'É’', 'u': 'ÊŒ',
            'th': 'Î¸', 'sh': 'Êƒ', 'ch': 'tÊƒ', 'ng': 'Å‹'
        }
        
        words = text.lower().split()
        phonemized_words = []
        for word in words:
            phonemized_word = word
            for key, value in phoneme_map.items():
                phonemized_word = phonemized_word.replace(key, value)
            phonemized_words.append(phonemized_word)
        
        return ' '.join(phonemized_words)
    
    def split_train_val(self):
        """åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
        print("ğŸµ æ­¥éª¤6: åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
        try:
            train_list_file = self.raw_dir / "train_list.txt"
            
            if not train_list_file.exists():
                raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒåˆ—è¡¨æ–‡ä»¶: {train_list_file}")
            
            # è¯»å–æ‰€æœ‰è¡Œ
            with open(train_list_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # éšæœºæ‰“ä¹±
            random.shuffle(lines)
            
            # æŒ‰1:10æ¯”ä¾‹åˆ’åˆ†éªŒè¯é›†
            val_ratio = 1 / 10
            val_size = max(1, int(len(lines) * val_ratio))
            
            val_lines = lines[:val_size]
            train_lines = lines[val_size:]
            
            # ä¿å­˜éªŒè¯é›†
            val_list_file = self.raw_dir / "val_list.txt"
            with open(val_list_file, 'w', encoding='utf-8') as f:
                f.writelines(val_lines)
            
            # æ›´æ–°è®­ç»ƒé›†ï¼ˆç§»é™¤éªŒè¯é›†éƒ¨åˆ†ï¼‰
            with open(train_list_file, 'w', encoding='utf-8') as f:
                f.writelines(train_lines)
            
            print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ - è®­ç»ƒé›†: {len(train_lines)} æ¡, éªŒè¯é›†: {len(val_lines)} æ¡")
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åˆ’åˆ†å¤±è´¥: {e}")
            raise
    
    def process(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹"""
        try:
            print(f"ğŸš€ å¼€å§‹å¤„ç†éŸ³é¢‘æ–‡ä»¶: {self.input_file}")
            
            # æ­¥éª¤1: Demucsåˆ†ç¦»
            self.run_demucs()
            
            # æ­¥éª¤2: é‡é‡‡æ ·
            resampled_audio = self.resample_audio()
            
            # æ­¥éª¤3: åˆ‡ç‰‡
            sliced_files = self.slice_audio(resampled_audio)
            
            # æ­¥éª¤4: è½¬å½•
            transcripts = self.transcribe_audio()
            
            # æ­¥éª¤5: éŸ³ç´ åŒ–
            self.phonemize_text(transcripts)
            
            # æ­¥éª¤6: åˆ’åˆ†æ•°æ®é›†
            self.split_train_val()
            
            print(f"ğŸ‰ {self.base_name} Process Successfully Finished!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.raw_dir}")
            
        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¤±è´¥: {e}")
            raise

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python audio_processor.py <input_wav_file>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    if not os.path.exists(input_file):
        print(f"é”™è¯¯: æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        sys.exit(1)
    
    processor = AudioProcessor(input_file)
    processor.process()

if __name__ == "__main__":
    main()
